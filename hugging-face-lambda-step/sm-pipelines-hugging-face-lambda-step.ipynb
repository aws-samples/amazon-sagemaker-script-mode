{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Pipelines Lambda Step and Hugging Face\n",
    "\n",
    "This notebook demonstrates how to use [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) to train and deploy a [Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html) transformer using a Lambda function invoked by SageMaker Pipelines Lambda Step. \n",
    "\n",
    "Prerequisites:  \n",
    "- Make sure your notebook environment has IAM managed policy `AmazonSageMakerPipelinesIntegrations` as well as `AmazonSageMakerFullAccess`\n",
    "- For SageMaker Studio, use the kernel `Python 3 (Data Science)`\n",
    "\n",
    "We'll start with some necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import CreateModelStep, RegisterModel\n",
    "\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline, PipelineExperimentConfig\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform some setup for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker.Session().boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sm_client\n",
    ")\n",
    "\n",
    "s3_prefix = \"hugging-face-pipeline-demo\"\n",
    "base_job_prefix = s3_prefix\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Before defining the pipeline, it is important to parameterize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.p3.2xlarge\"\n",
    ")\n",
    "training_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "# endpoint parameters\n",
    "endpoint_instance_type = ParameterString(\n",
    "    name=\"EndpointInstanceType\", default_value=\"ml.g4dn.xlarge\"\n",
    ")\n",
    "\n",
    "output_destination = \"s3://{}/{}/data\".format(bucket, s3_prefix)\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=False, expire_after=\"30d\")\n",
    "\n",
    "model_package_group_name = \"HuggingFaceModelPackageGroup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "The SKLearn Processing is used to invoke a SageMaker Processing job with a custom python script - `preprocessing.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=base_job_prefix + \"/preprocessing\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"ProcessDataForTraining\",\n",
    "    cache_config=cache_config,\n",
    "    processor=sklearn_processor,\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            destination=\"{}/train\".format(output_destination),\n",
    "            source=\"/opt/ml/processing/train\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            destination=\"{}/test\".format(output_destination),\n",
    "            source=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\",\n",
    "            destination=\"{}/test\".format(output_destination),\n",
    "            source=\"/opt/ml/processing/validation\",\n",
    "        ),\n",
    "    ],\n",
    "    code=\"./scripts/preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Hugging Face Estimator\n",
    "\n",
    "Use SageMaker's [Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) Estimator class to create train the Hugging Face [BERT](https://huggingface.co/distilbert-base-uncased) model.\n",
    "\n",
    "In the cell below, replace the container URI with the one for your region, as found in the table https://github.com/aws/deep-learning-containers/blob/master/available_images.md.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"huggingface\",\n",
    "                                        region=boto3.Session().region_name,\n",
    "                                        version=\"4.6.1\",\n",
    "                                        py_version=\"py36\",\n",
    "                                        base_framework_version=\"pytorch1.7.1\",\n",
    "                                        instance_type=\"ml.p3.2xlarge\",\n",
    "                                        image_scope=\"training\" )\n",
    "'''\n",
    "\n",
    "container = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.7.1-transformers4.6.1-gpu-py36-cu110-ubuntu18.04' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "}\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    image_uri=container,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./scripts\",\n",
    "    base_job_name=base_job_prefix + \"/training\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TrainHuggingFaceModel\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "A ProcessingStep is used to evaluate the performance of the trained model. Based on the results of the evaluation, the model is created and deployed.\n",
    "\n",
    "In the training job, the model was evaluated against the test dataset, the result of the evaluation was stored in the `model.tar.gz` file saved by the training job, the results of that evaluation are copied into a `PropertyFile` in this ProcessingStep so that it can be used in the ConditionStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_eval = ScriptProcessor(\n",
    "    image_uri=container,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=base_job_prefix + \"/evaluation\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AbaloneEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"HuggingfaceEvalLoss\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\",\n",
    "            destination=f\"s3://{bucket}/{s3_prefix}/evaluation_report\",\n",
    "        ),\n",
    "    ],\n",
    "    code=\"./scripts/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the model\n",
    "\n",
    "The trained model is registered into the Model Registry under a Model Package Group. The model is registered in the \"Approved\" state so that it can be deployed by the Lambda function. Registration will only happen if the output of the ConditionStep is true, i.e, the metrics being checked are within the threshold defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"HuggingFaceRegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g4dn.xlarge\", \"ml.m4.xlarge\"],\n",
    "    transform_instances=[\"ml.g4dn.xlarge\", \"ml.m4.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda Step\n",
    "\n",
    "The SageMaker SDK provides a Lambda helper class that can be used to create a Lambda function. This function is provided to the LambdaStep for invocation via the Pipeline. Alternatively, a pre-defined Lambda function can be provided to the LambdaStep. \n",
    "\n",
    "The SageMaker Execution Role requires the policy `AmazonSageMakerPipelinesIntegrations` to create the Lambda function and the Lambda function needs a role with policies for creating a SageMaker endpoint. \n",
    "\n",
    "A helper function in `iam_helper.py` is provided to create the Lambda role, the use the script, the notebook execution role must the required policy to create a role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "\"\"\"\n",
    "This Lambda function creates an Endpoint Configuration and deploys a model to an Endpoint. \n",
    "The name of the model to deploy is provided via the `event` argument\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\" \"\"\"\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # The name of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    bucket = event[\"bucket\"]\n",
    "    s3_prefix = event[\"s3_prefix\"]\n",
    "\n",
    "    container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    create_model_respose = sm_client.create_model(\n",
    "        ModelName=model_name, ExecutionRoleArn=role, Containers=[container]\n",
    "    )\n",
    "\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"VariantName\": \"variant1\",\n",
    "                \"ModelName\": model_name,\n",
    "                \"InstanceType\": \"ml.m5.xlarge\",\n",
    "                \"InitialInstanceCount\": 1\n",
    "            }\n",
    "        ],\n",
    "        AsyncInferenceConfig={\n",
    "            \"OutputConfig\": {\n",
    "                \"S3OutputPath\": f\"s3://{s3_bucket}/{s3_prefix}/output\",\n",
    "            },\n",
    "            \"ClientConfig\": {\n",
    "                \"MaxConcurrentInvocationsPerInstance\": 4\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Created Endpoint!\"),\n",
    "        \"other_key\": \"example_value\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IAM role using the `iam_helper` script or provide an existing IAM role to be used by the Lambda function\n",
    "\n",
    "from iam_helper import create_lambda_role\n",
    "\n",
    "lambda_role = create_lambda_role(\"lambda-deployment-role\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lambda Step\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = \"demo-hf-model\" + current_time\n",
    "endpoint_config_name = \"demo-hf-endpoint-config\" + current_time\n",
    "endpoint_name = \"demo-hf-endpoint-\" + current_time\n",
    "\n",
    "function_name = \"sagemaker-demo-hf-lambda-step\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a \n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(\n",
    "    output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String\n",
    ")\n",
    "output_param_2 = LambdaOutput(\n",
    "    output_name=\"body\", output_type=LambdaOutputTypeEnum.String\n",
    ")\n",
    "output_param_3 = LambdaOutput(\n",
    "    output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String\n",
    ")\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function \n",
    "# in the Lambda\n",
    "step_deploy_lambda = LambdaStep(\n",
    "    name=\"LambdaStepHuggingFaceDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": step_register.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": role,\n",
    "        \"bucket\": bucket,\n",
    "        \"s3_prefix\": s3_prefix\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluated loss of the hugging face model must be less than 0.3 for the condition to be True and the subsequent steps\n",
    "# to run\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"eval_loss\",\n",
    "    ),\n",
    "    right=0.3,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckHuggingfaceEvalLoss\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_deploy_lambda],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=f\"HuggingFaceDemoPipeline\" + \"-\" + current_time,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "After the previous cell completes, you can check whether the endpoint has finished deploying.  In SageMaker Studio, click on the tilted triange icon in the left toolbar, then select **Endpoints** in the drop down menu.  In the list of endpoints, click on the one with the name beginning `demo-hf-endpoint`, then click on **AWS settings**.  When the Status becomes InService, you can run the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "hf_predictor = sagemaker.predictor.Predictor(endpoint_name, \n",
    "                                             sagemaker_session=sagemaker_session,\n",
    "                                             serializer=JSONSerializer(),\n",
    "                                             deserializer=JSONDeserializer() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input1 = {\"inputs\":\"Although the movie had some plot weaknesses, it was engaging. Special effects were mind boggling.\"}\n",
    "\n",
    "hf_predictor.predict(sentiment_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input2 = {\"inputs\":\"There was some good acting, but the story was ridiculous. The other sequels in this franchise were better.\"}\n",
    "\n",
    "hf_predictor.predict(sentiment_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup Resources\n",
    "\n",
    "The following cell will delete the resources created by the Lambda function and the Lambda itself. \n",
    "Deleting other resources such as the S3 bucket and the IAM role for the Lambda function are the responsibility of the notebook user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Lambda function\n",
    "func.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete the EndpointConfig\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete the model\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
